{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Tuning ML Models\n",
    "- Hyperparameter Tuning\n",
    "    - Algorithms for model tunign\n",
    "    - Sagemaker automated parameter tuning\n",
    "    - BERT Automated tuning\n",
    "    \n",
    "    - warm start hyperparameter turning\n",
    "        - speedup the tuning process\n",
    "    - Check point in machine learning\n",
    "        - managed spot training; to reduce the cost of training\n",
    "        \n",
    "    - Distirbuted training challenges\n",
    "        - Tune distributed training stragegies\n",
    "            - data panelism\n",
    "            - model panelism\n",
    "            \n",
    "    - Bring your own container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pics/courser outline.png'>\n",
    "\n",
    "- challenges in training and tuning large machine learning models while optimizing cost\n",
    "\n",
    "- Popular algorithms for hyperparameter tuning (Automatic)\n",
    "    - 4 algorithms\n",
    "    - <u>Grid search</u>\n",
    "        - specify parameters to tune\n",
    "        - specify the range of the values for each parameter\n",
    "        - the algorithm will train the model of every combination and find the best parameter\n",
    "\n",
    "        - Pros\n",
    "            - Explore all possible combintions\n",
    "            - works for small number of parameters\n",
    "        - Cons\n",
    "            - time consuming for large number of parameters\n",
    "            - doesn't scale for large number of parameters\n",
    "        \n",
    "    - <u>Random search</u>\n",
    "        - Specify the hyperparmter to tune\n",
    "        - Define the search space and stop criteria\n",
    "            - some stop criteria may be,\n",
    "                - time elapsed\n",
    "                - maximum number of models to be trained\n",
    "                \n",
    "        - Test random combinations within the search space\n",
    "        - select the best combination\n",
    "        \n",
    "        - Pros\n",
    "            - Faster than the grid search\n",
    "        - Cons\n",
    "            - May miss the best performing hyperparameters\n",
    "        \n",
    "    - <u>Bayesian optimization</u>\n",
    "        - Hyperparameter is treated as a regression problem\n",
    "        - Start with random hyperprameters\n",
    "        - Narrow down the search space around a better performing hyperprameter\n",
    "        \n",
    "        - Pros\n",
    "            - more efficient in finding the best hyperparameter\n",
    "            \n",
    "        - Cons\n",
    "            - requires sequential execution\n",
    "            - migh get stuck in local minima\n",
    "            \n",
    "            - <img src='./pics/local minima.png'>\n",
    "        \n",
    "            - this is a prominent probelm when we are using gradient discent for opimizing the probelm\n",
    "                \n",
    "    - <u>Hyperband</u>\n",
    "        - <b>Bandit-based</b> approach (exploitations, explorations)\n",
    "        - Start from random hyperparameters\n",
    "        - Explore set of hyperparameters for few iterations (after exploration abandon half of the least performing sets)\n",
    "        - choose best and explore longer\n",
    "        - Repeat until max_iterations reached or one candidate left\n",
    "        \n",
    "        - Pros\n",
    "            - Spends time efficiently\n",
    "        - Cons\n",
    "            - might discard good candidates early that converge slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune a BERT-based Text Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT model Hyper parameter tuning\n",
    "    - Run multiple training with diff hyperparameters\n",
    "    - the tunign ob accepts\n",
    "        - Turning strategy\n",
    "            - sagemaker natively supports 2 strategies\n",
    "                - Random\n",
    "                - Baysian\n",
    "                \n",
    "        - objective metrics\n",
    "            - an example will be maximize the validation accuracy\n",
    "            \n",
    "        <img src='./pics/Turning strategy.png'>\n",
    "        \n",
    "    - Use case\n",
    "        - Tune the BERT Text Classifier\n",
    "            - Tuning strategy\n",
    "                - Random\n",
    "            - Objective Metrics\n",
    "                - Maximize the validation accuracy\n",
    "                \n",
    "            - <b>Hyperparametes</b>\n",
    "                - learning rate?\n",
    "                - train_batch_size\n",
    "                \n",
    "    - The step involves in the process\n",
    "    \n",
    "        - create pytorch estimator\n",
    "            - define fixed hyperparameters that we donot ant to tune during the process\n",
    "            - create a dictionary\n",
    "            - <img src='./pics/hyperparameter dictionary.png'>\n",
    "            \n",
    "            - pass the fixed hyperparamerters to the estimator\n",
    "            \n",
    "        - create HPT(Hyperparameter Tuning) job\n",
    "            - define the tunable hyperparameres\n",
    "            - <img src='./pics/tunable hyperparameter dictionary.png'>\n",
    "            - specified the name of the hpyerparameters and the range of the values, along with that the hyperparameter tyep is also sepcified. The type may be one of the following,\n",
    "                - categorical\n",
    "                - continous\n",
    "                - integer \n",
    "                \n",
    "            - to test values insted of range (the pic `train_batch_size` parameter, specify it as a categorical parameter\n",
    "            - integer parameter\n",
    "                - <img src='./pics/integer parameter.png'>\n",
    "                - all values between 16 and 1024 has to be explored in the above case\n",
    "            - if the values to be explored is large use logarithamic scale to optimize the process (look at the pic above)\n",
    "            \n",
    "            - continous parameter \n",
    "                - learning rate may be a continous parameter\n",
    "                - <img src='./pics/continous parameter.png'>\n",
    "                \n",
    "            - Now create the Hyperparameter tuner object\n",
    "                - <img src='./pics/Hyperparameter tuner object.png'>\n",
    "                    - estimator is the estimator we defined for training\n",
    "                    - hyperparameter rage is the tunable hyperparameter defined\n",
    "                    - give the objective type\n",
    "                    \n",
    "            - `tuner.fit(inputs={...}, ...)`\n",
    "                \n",
    "        - analyze the results\n",
    "            - <img src='./pics/tuner result.png'>\n",
    "            \n",
    "       - Warm start\n",
    "           - start from a perviously trained hyperparameter job\n",
    "               - to change the hyperparameter range\n",
    "               - explore new set of hyperparameters\n",
    "               \n",
    "               - 2 jobs\n",
    "                   - IDENTICL_DATA_AND_ALGORITHM\n",
    "                       - same data and algorithm of the parent tuning job\n",
    "                       - can update the hyperparameter range and the maximum number of tuning jobs\n",
    "                   - TRANSFER_LEARNING\n",
    "                       - updated training data and a diff version of the training algorithm\n",
    "                       - warm start in code\n",
    "                       - <img src='./pics/warm start code.png'>\n",
    "                           - warm start config specifies is it an idendital data and code or a trasfer leaning problem in the warm start config \n",
    "                           - pass the warm start config to the Hyperparameter tuner job `warm_start_config` parameter\n",
    "\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- select a small number of hyperparameters\n",
    "    - computational complexity directly prepotional to the number of hyperparamter to tune\n",
    "        - sagemaker allow upto 20 hyperparameter to tune at a time\n",
    "- select a small range of values to tune during the tuning\n",
    "       \n",
    "- Enable warm smart\n",
    "- Enable early stop\n",
    "    - stop the training when the objective metric is not improving\n",
    "- select small list of concurrent training jobs\n",
    "    - turning can be done parallel\n",
    "    - but the turning is a sequenctial process, one job is better than the other can be determined by completing the previous process\n",
    "    - so choose a small no of concurrent process\n",
    "    \n",
    "- Right size compute resources\n",
    "    - empherical testing\n",
    "    - coud watch metric for compute resource utalization\n",
    "    - Debugger will tell the resouce utalization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>ML model training challenges</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing\n",
    "\n",
    "- ML training uses checkpoints to reduce the cost of training\n",
    "- Checkpointing\n",
    "    - way to save the current state of model during training\n",
    "    - the training can be resumed from a known state\n",
    "        - in case of an unforeseen event (os failure)\n",
    "        \n",
    "    - checkpoints are the snapshots of the model\n",
    "        - model archtecture\n",
    "        - model weights\n",
    "        - training configureations\n",
    "            - no of epochs\n",
    "            - loss and optimizers\n",
    "            - other metadata\n",
    "        - optimizer state\n",
    "        \n",
    "- Importtant points while checkpointing\n",
    "    - fequeceny and number of checkpointing\n",
    "    - storage vs model infromation trade off\n",
    "    \n",
    "- <u><b>Amazon SageMaker Managed Spot</b></u>\n",
    "    - reduce the training cost\n",
    "    - unused spot instances at a discounted price\n",
    "    - use spot instances to \n",
    "        - train\n",
    "        - hyperparameter tuning\n",
    "        - use checkpoints to resume training\n",
    "    \n",
    "- How it works\n",
    "    - start training in a docker container\n",
    "        - script: train.py\n",
    "        - the spot instance will stop with 2 minute warning\n",
    "        - the spot instance should have the ability to save the checkpoint (/opt/ml/checkpoints)\n",
    "   - backup the docker to an s3\n",
    "   - ability to resume from the checkpoints\n",
    "   - <img src='./pics/checkpoints.png'>\n",
    "        \n",
    "    - in case the spot instance is terminated, the sagemaker managed spot polls for the spots and once the spot is find the managed spot resumes the trianing and by trasnfering the checkpoints and the data to the new instnace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Strategies\n",
    "\n",
    "- Training at scale\n",
    "    - increased the data volume\n",
    "    - increased model size and complexity\n",
    "    \n",
    "- it is difficult to fit a very large model into a single computer memory\n",
    "- distributed compute is a way to address the scale challenges\n",
    "- <img src='./pics/distributed training.png'>\n",
    "    \n",
    "- the load is distirbuted among\n",
    "    - single computer multiple gpus/cpus\n",
    "    - muliptle computer multiple gpus/cpus\n",
    "    \n",
    "- There are 2 distributed training strategies\n",
    "    - data parallelism\n",
    "        - trainging data is split up among the training node\n",
    "        - model replicated among the nodes\n",
    "    - model parallelism\n",
    "        - the model is split up among the nodes\n",
    "        - data is replicated among the nodes\n",
    "    - <img src='./pics/distribute strategies.png'>\n",
    "- code below- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parallelism\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = 'train.py',\n",
    "    role = sagemaker.get_execution_role(),\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    instance_count=3,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    distribution={'smdistributed':{'dataprallel':{'enabled':True}}}\n",
    ")\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "\n",
    "# model parallelism\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = 'train.py',\n",
    "    role = sagemaker.get_execution_role(),\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    instance_count=3,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    distribution={'smdistributed':{'modelparellel':{'enabled':True}}}\n",
    ")\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distiributeion overhead\n",
    "    - internode communication\n",
    "    \n",
    "- when to use diff strategies\n",
    "    - model fit on a single node\n",
    "    - use data prallelism\n",
    "    - model doesnot fit\n",
    "    - resize the model\n",
    "        - hyperparameter tuning\n",
    "            - number of layers \n",
    "            - the optimzer to use\n",
    "            - these two parameters has a considerable effect on the final model size\n",
    "        - Reduce the batch size\n",
    "            - incrementally reduce the batch size\n",
    "        - Reduce the model input size\n",
    "            - consider the model is taking a text input consider embedding the text with a low dimensional embedding vector\n",
    "            - in case of a image input, try to reduce the image resolution\n",
    "    -even after the experimentation md\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Algorithms with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
