{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Model Deployment and integration</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- model deployment options\n",
    "- how to choose the right option\n",
    "\n",
    "- strategy to deploy model\n",
    "    - minimize risk\n",
    "        - A/B Testing\n",
    "        - multi-armed bandits\n",
    "        \n",
    "- integrating models\n",
    "    - how to use model\n",
    "   \n",
    "- monitor the model\n",
    "    - sagemaker model monitor\n",
    "    \n",
    "- Lab\n",
    "    - setting monitors for deployment\n",
    "    - setting A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment Overview\n",
    "\n",
    "- <img src='./pics/machine learning workflow.png'>\n",
    "\n",
    "- model deployment\n",
    "    - clould\n",
    "        - real-time inferance\n",
    "        - batch inferance\n",
    "        \n",
    "- Deployment option (Real-time inferance)\n",
    "- <img src='./pics/deploying model.png'>\n",
    "\n",
    "- Batch inferance\n",
    "- <img src='./pics/batch inferance.png'>\n",
    "    - running a batch job against a batch request\n",
    "    - store prediction in a db\n",
    "    \n",
    "- Deployment of model in Edge\n",
    "    - deploy model closer to users\n",
    "    - in case of poor network connectivity\n",
    "    \n",
    "    - procedure\n",
    "        - train model in somewhere\n",
    "        - optimize the model for deployment\n",
    "            - package the model to run on smaller devices\n",
    "            - sagemaker neo to compile the model to run at the edge\n",
    "        - typical example: manufacturing defects at the production end\n",
    "        - inferance will be send to cloud\n",
    "            - for further analysis\n",
    "            - for training and optimizing the model\n",
    "- <img src='./pics/edge deployment.png'>\n",
    "\n",
    "## Deployment options\n",
    "- <img src='./pics/deployment options.png'>\n",
    "\n",
    "    - Transient environment: computation and storage will be consumed during the operation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment Strategies\n",
    "\n",
    "- deploy new/updated model\n",
    "- <img src='./pics/model deployment strategies.png'>\n",
    "    - if you have a new model, you dont want to deploy the model that disrupts the service\n",
    "    - you may want to monitor that model for a period to assess the performace of the model, and if there is any issue may need to role back that model\n",
    "   \n",
    "- Deployment strategies\n",
    "    - Blue/Green\n",
    "    - Shadow/Challenger\n",
    "    - Canary\n",
    "    - A/B\n",
    "    - Multi-Armed Bandits\n",
    "    \n",
    "- Blue/Green Deployment\n",
    "    - swap the prediction trafic to new model\n",
    "    - advantage: easy to roll back and reduce the downtime\n",
    "    - <img src='./pics/Blue Green Deployment.png'>\n",
    "    - risk: if the new model is not performing well you are serving bad prediction to the 100% of the trafic\n",
    "    \n",
    "- Shadow/Challenger Deployment\n",
    "    - parellel pediction request traffic\n",
    "    - validate the new version without impact\n",
    "    - <img src='./pics/shadow channelger deployment.png'>\n",
    "    - the predition from model 1 will be send to the user\n",
    "    - prediction from model 2 is caputred for analysis\n",
    "    - once model 2 is find to be confortable then we can sever the predictions form model verison 2 to users\n",
    "    \n",
    "- Canary Deployment\n",
    "    - split traffic\n",
    "    - target smaller sepecific user/groups\n",
    "    - shorter validation cycles\n",
    "    - minimize risk of low performing model    \n",
    "    - <img src='./pics/canery deployment.png'>\n",
    "\n",
    "- A/B Deployment\n",
    "    - splitting traffic\n",
    "        - similar to canery\n",
    "        - splitting may be based on group or random splitting\n",
    "    - Target larger users/groups ~OR~ Distiribute % of traffic\n",
    "    - Longer validation cycles\n",
    "    - minimize risk of low performing models\n",
    "    - <img src='./pics/AB deployment.png'>\n",
    "    \n",
    "- all the approches that has been covered are static approches\n",
    "\n",
    "- Multi-Armed Bandits\n",
    "    - Dynamic approch\n",
    "        - machine learning to decide how and when to rute traffic b/w models\n",
    "    -  Use Reinforcement learning to shit traffic to the winning model\n",
    "    - however there will be traffic to non-winning model since early winners are not the best model\n",
    "    - experiment manager: a reinforcement model to determine the traffic b/w model\n",
    "    - reward metircs\n",
    "        - more traffic to the winning model (exploitation)\n",
    "        - exploration: can the loosing model can catch-up with the winning model\n",
    "        \n",
    "    - <img src='./pics/multi-armed strategies.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Hosting: Real-Time Inferance\n",
    "\n",
    "- <img src='./pics/sagemaker endpoints.png'>\n",
    "\n",
    "- model serving stack\n",
    "- Hosting stack\n",
    "    - web server that interact with the model\n",
    "    - client application will interact with real time invoking\n",
    "    - request will send once you invoke the model\n",
    "        - request sent though a load balancer\n",
    "        - diff builtin serializers and deserializers\n",
    "        \n",
    "   - we can choose,\n",
    "       - instance type (resounces you need to predict)\n",
    "       - instance size\n",
    "       - number of instances\n",
    "       - autoscaling options\n",
    " \n",
    "- 3 Options to select model\n",
    "    - builtin model\n",
    "    - Script mode\n",
    "    - Docker\n",
    "    \n",
    "- Builtin Model\n",
    "- <img src='./pics/builtin model hosting.png'>\n",
    "- you need,\n",
    "    - the model container\n",
    "    - stored model artifacts in s3 (the trained model parameters)\n",
    "\n",
    "- Script mode\n",
    "- <img src='./pics/script mode hosting.png'>\n",
    "- same as above, but bring the inferance code\n",
    "\n",
    "- Docker (Bringing you own model)\n",
    "- <img src='./pics/docker hosting.png'>\n",
    "       \n",
    "- AWS will distirbute the containers in various availability zones for high availability\n",
    "\n",
    "\n",
    "## Autoscaling \n",
    "- Why?\n",
    "    - meet the demand of the workload\n",
    "    - cost optimization\n",
    "    \n",
    "- <img src='./pics/autoscaling.png'>\n",
    "\n",
    "- cloudwatch capture metrics like,\n",
    "    - utalization metrics\n",
    "    - invocation metrics: no of invocation has been send against a ML hosting\n",
    "        - this is the default autoscaling metric. you  can change the metic if you want such as cpu utalization\n",
    "        \n",
    "- <img src='./pics/scaling policy.png'>\n",
    "\n",
    "- once the instances are up, the load balancer can distribute the load among the various instances.\n",
    "- cooldown policy: to scale down the model\n",
    "    - time to wait before scaling down once scaled up (in sec)\n",
    "    \n",
    "- cooldown period: the time in second a sacle in can take place after a scale in happen \n",
    "\n",
    "### how to do the scaling? (cell below)\n",
    "- reigester a scalable target\n",
    "    - an aws resouce to scale: here it is the sagemaker\n",
    "    - define the scaling policy\n",
    "    - Apply autoscaling policy\n",
    "        - apply the autoscale to the endpoint\n",
    "        - TargetTrackingScaling is the spcific policy type supported by the sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Scalable Target\n",
    "autoscale.register_scalable_target(\n",
    "    ServiceNameSpace='sagemaker'\n",
    "    ResourceId='endpoint/'+endpoint_name,\n",
    "    ScalabelDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2,\n",
    "    RoleARN=role,\n",
    "    SuspendedState={\n",
    "        \"DynamicScalingInSuspended\": False,\n",
    "        \"DynamicScaleingOutSuspended\": False:\n",
    "        \"ScheduleScalingSuspended\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the Scaling Policy\n",
    "scaling_policy = {\n",
    "    \"TargetValue\": 2.0,\n",
    "    \"PredefinedMetricSpecification\":{ # scaling metirc\n",
    "        \"PredifinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "    },\n",
    "    \"ScaleOutCooldown\": 60, # wait time, in second, before beginning another scale out activty after last one completes\n",
    "    \"ScaleInCooldown\": 300, # wait time in second, before beginning another scale in activity after the last one completes\n",
    "}\n",
    "\n",
    "# apply scaling policy\n",
    "autoscale.put_scaling_policy(\n",
    "    PolicyName=...,\n",
    "    ServiceNameSpace='sagemaker',\n",
    "    ResouceId=\"endpoint/\"+endpoint_name,\n",
    "    ScaleableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration=scaling_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Endpoints\n",
    "\n",
    "- <img src='./pics/multi model endpoint.png'>\n",
    "\n",
    "- multiple model behind same endpoints\n",
    "    - sagemaker dynamically invoke various models\n",
    "    - the invoking is based on the client request\n",
    "        - here in the pic, the client application is invoking the model 1\n",
    "    - the model will be loaded till the resources exhusted \n",
    "    - all the models share an endpoint will share the same container\n",
    "    \n",
    "## Inferace Pipeline\n",
    "\n",
    "- <img src='./pics/inferance pipeline.png'>\n",
    "- the inferace pipeline run in a sequential manner behind the same endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker: Real-Time Inferance Production Variants\n",
    "\n",
    "- production varienets\n",
    "    - A/B Testing (lab)\n",
    "    - Canery  testing\n",
    "    \n",
    "- <img src='./pics/production varient.png'>\n",
    "\n",
    "- production varient can be used for Cannery testing and A/B testing\n",
    "- <img src='./pics/production varient for canery.png'>\n",
    "\n",
    "- The canery user group will routed to the varient B model\n",
    "    - this is achieved programatically, by invoking model B when the client application invokest the model\n",
    "    \n",
    "- A/B testing (LAB for the week)\n",
    "- <img src='./pics/production varient for AB.png'>\n",
    "    - the traffic is splitted equally\n",
    "    - you can use programtic traffic to route a particular traffic to a particular model\n",
    "    - In A/B testing\n",
    "        - a larger group is participating in the model testing\n",
    "        - for a longer period of time\n",
    "\n",
    "## Septs for A/B testing (see the cell below)\n",
    "\n",
    "- construct Docker image URIs\n",
    "- crate 2 model object\n",
    "    - <img src='./pics/sagemaker models.png'>\n",
    "    \n",
    "- create production varients\n",
    "    - <img src='./pics/create production varients.png'>\n",
    "    - 50% of the varient will flow to model A while other 50% flow to model B\n",
    "    \n",
    "- create the endpoint configuration\n",
    "- create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct docker image URIs\n",
    "import sagemaker\n",
    "\n",
    "inferace_image_uri = sagemaker.image_uris.retireve(\n",
    "    framework = ..., # pytorch, tensorflow etc\n",
    "    version='1.6.0',\n",
    "    instance_type='ml.p5.xlarge',\n",
    "    py_version='py3',\n",
    "    image_scope='inferance'\n",
    ")\n",
    "\n",
    "# create model\n",
    "sm.create_model(\n",
    "    name = model_name_a,\n",
    "    ....\n",
    ")\n",
    "\n",
    "sm.create_model(\n",
    "    name = model_name_b,\n",
    "    ....\n",
    ")\n",
    "\n",
    "# create production varients\n",
    "from sagemaker.session import production_variant\n",
    "\n",
    "varientA = production_variant(\n",
    "    model_name=...,\n",
    "    instance_type=...,\n",
    "    initial_instance_coutn=1,\n",
    "    variant_name='VariantA',\n",
    "    inital_weight=50,\n",
    ")\n",
    "\n",
    "# endpoint configuration\n",
    "endpoint_config = sm.create_endpoint_config(\n",
    "            EndpointConfigName=...,\n",
    "            ProductionVriants=[VarientA, VarientB]\n",
    ")\n",
    "\n",
    "# create endpoint\n",
    "endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=...,\n",
    "    EndpointConfigName=...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Batch Trasnform: Batch Inference\n",
    "\n",
    "- SageMaker Batch Trasform\n",
    "- workflow\n",
    "    - <img src='./pics/batch workflow.png'>\n",
    "    \n",
    "    - package the model\n",
    "    - <img src='./pics/package the model batch.png'>\n",
    "    \n",
    "    - create the transformer\n",
    "    - <img src='./pics/batch trasformer.png'>\n",
    "    \n",
    "    - start the trasformation job\n",
    "    - <img src='./pics/run trasformer.png'>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Model integration and monitoring</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring ML Workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring using Amazon Sagemaker Model Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
